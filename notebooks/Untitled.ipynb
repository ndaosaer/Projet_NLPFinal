{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866d70f2-84f7-4960-bc7a-974a9b5fe68b",
   "metadata": {},
   "source": [
    "#### ========================================\n",
    "### NOTEBOOK 2 : PR√âTRAITEMENT DU TEXTE\n",
    "### Projet Classification de CV - LiveCareer\n",
    "### ========================================\n",
    "\n",
    "### Pr√©traitement du Texte des CV\n",
    " \n",
    "##### **Ce notebook utilise la classe TextCleaner de src/preprocessing/text_cleaner.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc21339-c1c1-406e-a118-76d9158de5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be201eb1-a399-4482-8bbb-39717bf22832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le dossier src au path Python\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Importer notre classe TextCleaner depuis src/\n",
    "from src.preprocessing.text_cleaner import TextCleaner\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis!\")\n",
    "print(f\"   - TextCleaner import√© depuis: src/preprocessing/text_cleaner.py\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Chargement des Donn√©es Brutes\n",
    "\n",
    "# %%\n",
    "# Charger le dataset original\n",
    "df = pd.read_csv('../data/raw/resume_dataset.csv')\n",
    "\n",
    "print(f\"üìä Dataset charg√©:\")\n",
    "print(f\"   - Nombre de CV: {len(df)}\")\n",
    "print(f\"   - Colonnes: {df.columns.tolist()}\")\n",
    "\n",
    "# Identifier les colonnes\n",
    "text_col = 'Resume'  # Ajustez selon votre dataset\n",
    "category_col = 'Category'\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"\\nüìÑ Exemple de CV brut (500 premiers caract√®res):\")\n",
    "print(\"=\"*80)\n",
    "print(df[text_col].iloc[0][:500] + \"...\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Initialisation du TextCleaner\n",
    "\n",
    "# %%\n",
    "# Cr√©er une instance du TextCleaner\n",
    "# (La classe est d√©finie dans src/preprocessing/text_cleaner.py)\n",
    "cleaner = TextCleaner(\n",
    "    lowercase=True,\n",
    "    remove_urls=True,\n",
    "    remove_emails=True,\n",
    "    remove_phone_numbers=True,\n",
    "    remove_numbers=False,  # Garder les ann√©es d'exp√©rience\n",
    "    remove_punctuation=True,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True,\n",
    "    stem=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ TextCleaner initialis√© avec les param√®tres:\")\n",
    "print(f\"   - Stopwords: {len(cleaner.stop_words)} mots\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Test sur un Exemple\n",
    "\n",
    "# %%\n",
    "# Prendre un CV exemple\n",
    "sample_cv = df[text_col].iloc[0]\n",
    "\n",
    "print(\"üìù AVANT NETTOYAGE:\")\n",
    "print(\"=\"*80)\n",
    "print(sample_cv[:500] + \"...\")\n",
    "\n",
    "# Nettoyer avec notre classe\n",
    "cleaned_sample = cleaner.clean_text(sample_cv)\n",
    "\n",
    "print(\"\\n‚ú® APR√àS NETTOYAGE:\")\n",
    "print(\"=\"*80)\n",
    "print(cleaned_sample[:500] + \"...\")\n",
    "\n",
    "print(f\"\\nüìä STATISTIQUES:\")\n",
    "print(f\"   Longueur originale: {len(sample_cv)} caract√®res\")\n",
    "print(f\"   Longueur nettoy√©e:  {len(cleaned_sample)} caract√®res\")\n",
    "print(f\"   R√©duction: {((len(sample_cv) - len(cleaned_sample)) / len(sample_cv)) * 100:.1f}%\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Nettoyage de Tout le Dataset\n",
    "\n",
    "# %%\n",
    "# Utiliser la m√©thode clean_dataframe de notre classe\n",
    "df = cleaner.clean_dataframe(df, text_column=text_col, output_column='cleaned_text')\n",
    "\n",
    "# Afficher les premi√®res lignes\n",
    "print(\"\\nüëÄ Aper√ßu du DataFrame nettoy√©:\")\n",
    "df[[category_col, 'cleaned_text']].head()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Analyse du Nettoyage\n",
    "\n",
    "# %%\n",
    "# Calculer les longueurs avant/apr√®s\n",
    "df['length_before'] = df[text_col].astype(str).apply(len)\n",
    "df['length_after'] = df['cleaned_text'].apply(len)\n",
    "df['reduction_percent'] = ((df['length_before'] - df['length_after']) / df['length_before']) * 100\n",
    "\n",
    "print(\"\\nüìä STATISTIQUES DE NETTOYAGE:\")\n",
    "print(\"=\"*60)\n",
    "print(df[['length_before', 'length_after', 'reduction_percent']].describe())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Visualisation de l'Impact du Nettoyage\n",
    "\n",
    "# %%\n",
    "# Cr√©er des visualisations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Distribution longueurs avant\n",
    "axes[0, 0].hist(df['length_before'], bins=50, color='lightcoral', \n",
    "               edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Longueur (caract√®res)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Fr√©quence', fontweight='bold')\n",
    "axes[0, 0].set_title('Distribution AVANT Nettoyage', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axvline(df['length_before'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Moyenne: {df[\"length_before\"].mean():.0f}')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Distribution longueurs apr√®s\n",
    "axes[0, 1].hist(df['length_after'], bins=50, color='lightgreen', \n",
    "               edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Longueur (caract√®res)', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Fr√©quence', fontweight='bold')\n",
    "axes[0, 1].set_title('Distribution APR√àS Nettoyage', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axvline(df['length_after'].mean(), color='green', linestyle='--',\n",
    "                   label=f'Moyenne: {df[\"length_after\"].mean():.0f}')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. R√©duction en pourcentage\n",
    "axes[1, 0].hist(df['reduction_percent'], bins=50, color='skyblue',\n",
    "               edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('R√©duction (%)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Fr√©quence', fontweight='bold')\n",
    "axes[1, 0].set_title('Taux de R√©duction', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axvline(df['reduction_percent'].mean(), color='blue', linestyle='--',\n",
    "                   label=f'Moyenne: {df[\"reduction_percent\"].mean():.1f}%')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Scatter avant vs apr√®s\n",
    "sample_df = df.sample(min(1000, len(df)))\n",
    "axes[1, 1].scatter(sample_df['length_before'], sample_df['length_after'],\n",
    "                  alpha=0.5, c='purple')\n",
    "axes[1, 1].set_xlabel('Longueur AVANT', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Longueur APR√àS', fontweight='bold')\n",
    "axes[1, 1].set_title('Corr√©lation Avant/Apr√®s', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "# Ligne de r√©f√©rence\n",
    "max_val = max(df['length_before'].max(), df['length_after'].max())\n",
    "axes[1, 1].plot([0, max_val], [0, max_val], 'r--', alpha=0.5, label='Pas de r√©duction')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/cleaning_impact.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique sauvegard√© dans outputs/plots/cleaning_impact.png\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Analyse des Mots les Plus Fr√©quents\n",
    "\n",
    "# %%\n",
    "# Utiliser la m√©thode get_word_frequency de notre classe\n",
    "word_freq = cleaner.get_word_frequency(df['cleaned_text'], top_n=30)\n",
    "\n",
    "print(\"\\nüî§ TOP 30 MOTS APR√àS NETTOYAGE:\")\n",
    "print(\"=\"*60)\n",
    "for i, (word, count) in enumerate(word_freq.items(), 1):\n",
    "    print(f\"{i:2d}. {word:20s} : {count:7d}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Comparaison Avant/Apr√®s sur un Exemple\n",
    "\n",
    "# %%\n",
    "# Utiliser la m√©thode visualize_cleaning_effect de notre classe\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä EXEMPLE DE NETTOYAGE D√âTAILL√â\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_idx = 5\n",
    "original = df[text_col].iloc[sample_idx]\n",
    "cleaned = df['cleaned_text'].iloc[sample_idx]\n",
    "\n",
    "cleaner.visualize_cleaning_effect(original, cleaned)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Sauvegarde des Donn√©es Nettoy√©es\n",
    "\n",
    "# %%\n",
    "# Sauvegarder le DataFrame nettoy√©\n",
    "output_path = '../data/processed/resume_cleaned.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"üíæ Donn√©es nettoy√©es sauvegard√©es dans: {output_path}\")\n",
    "print(f\"   Colonnes sauvegard√©es: {df.columns.tolist()}\")\n",
    "print(f\"   Nombre de lignes: {len(df)}\")\n",
    "\n",
    "# Version compacte (seulement les colonnes essentielles)\n",
    "df_compact = df[[category_col, 'cleaned_text']].copy()\n",
    "df_compact.to_csv('../data/processed/resume_cleaned_compact.csv', index=False)\n",
    "\n",
    "print(f\"\\nüíæ Version compacte sauvegard√©e dans: resume_cleaned_compact.csv\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. R√©sum√©\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä R√âSUM√â DU PR√âTRAITEMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ DONN√âES TRAIT√âES:\n",
    "   - CV nettoy√©s: {len(df)}\n",
    "   - R√©duction moyenne: {df['reduction_percent'].mean():.1f}%\n",
    "   - Longueur moyenne avant: {df['length_before'].mean():.0f} caract√®res\n",
    "   - Longueur moyenne apr√®s: {df['length_after'].mean():.0f} caract√®res\n",
    "\n",
    "üìÅ FICHIERS CR√â√âS:\n",
    "   - data/processed/resume_cleaned.csv (complet)\n",
    "   - data/processed/resume_cleaned_compact.csv (compact)\n",
    "   - outputs/plots/cleaning_impact.png\n",
    "\n",
    "üîß CLASSE UTILIS√âE:\n",
    "   - src/preprocessing/text_cleaner.py ‚Üí TextCleaner\n",
    "\n",
    "üéØ PROCHAINES √âTAPES:\n",
    "   1. Feature Extraction (TF-IDF) ‚Üí Notebook 03\n",
    "   2. Utilise: src/preprocessing/feature_extractor.py\n",
    "   \n",
    "‚úÖ Passez au Notebook 03_feature_extraction.ipynb\n",
    "\"\"\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# ## ‚úÖ FIN DU PR√âTRAITEMENT\n",
    "# \n",
    "# **Points cl√©s:**\n",
    "# - ‚úÖ Utilis√© TextCleaner depuis src/preprocessing/text_cleaner.py\n",
    "# - ‚úÖ Pas de duplication de code\n",
    "# - ‚úÖ Fichier resume_cleaned.csv cr√©√©\n",
    "# - ‚úÖ Pr√™t pour l'extraction de features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
